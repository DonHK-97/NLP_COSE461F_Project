{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"name":"Deep_ResNet.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"1855f521"},"source":["import torch\n","import torch.nn as nn\n","import fastai.layers as L\n","import torch.nn.functional as F\n","from collections import OrderedDict\n","\n","torch.backends.cudnn.deterministic = True\n","\n","def embed_layer(weights_matrix):\n","\n","  num_embeddings, embedding_dim = weights_matrix.shape\n","  weights_matrix = torch.from_numpy(weights_matrix)\n","  \n","  emb_layer = nn.Embedding(num_embeddings, embedding_dim, padding_idx=1).to('cuda')\n","  emb_layer.load_state_dict({'weight': weights_matrix})\n","  emb_layer.weight.require_grad = True\n","\n","  return emb_layer\n","\n","class dropsampleMergeLayer(nn.Module):\n","  def __init__(self, inplanes, dropout, dense):\n","    super().__init__()\n","    self.downsample = L.conv1d(inplanes, inplanes*2, stride=2)\n","    self.dropout = dropout\n","\n","  def forward(self, x):\n","    x_drop = F.dropout(x, p=self.dropout)\n","    return x_drop + self.downsample(x.orig)\n","\n","def transition(inplanes, dropout, dense = True):\n","\n","  trans = L.SequentialEx(\n","      L.conv1d(inplanes, inplanes*2, ks = 3, stride=2, padding = 1),\n","      dropsampleMergeLayer(inplanes, dropout = dropout, dense= dense))\n","\n","  return trans\n","\n","def resLayer(inplanes, leaky, self_attention):\n","  \n","  conv_kwargs = {'is_1d' : True, 'self_attention' : self_attention, 'leaky' : leaky}\n","  return L.res_block(inplanes, **conv_kwargs)\n","\n","class ResNet(nn.Module):\n","  def __init__(self, weights_matrix, layers = (3, 3, 6, 6), inplanes = 32, \n","               dropout = 0.35, leaky = 0.01, embed_dim = 50, self_attention = True):\n","    \n","    super(ResNet, self).__init__()\n","    self.embedding = embed_layer(weights_matrix)\n","    self.init_conv = L.conv1d(1, inplanes, ks=(3, embed_dim), stride=1, padding=(1,0), bias=False)\n","    self.dropout = dropout\n","    self.features = nn.Sequential(OrderedDict([\n","                                               ('init_conv', self.init_conv),\n","                                               ('init_norm', nn.BatchNorm1d(inplanes)),\n","                                               ('init_relu', nn.LeakyReLU(inplace=True))]))\n","        \n","    num_features = inplanes\n","    \n","    for i, layer in enumerate(layers):\n","      self.features.add_module('resblock%d' %(i+1), \n","                               self._make_block(num_features, layer, leaky=leaky, self_attention= self_attention))\n","      if i!= len(layers)-1:\n","        self.features.add_module('transition%d' %(i+1), \n","                                 transition(num_features, dropout = self.dropout))\n","        num_features = num_features*2\n","\n","    self.leakyrelu = L.relu(inplace=True, leaky=leaky)\n","    self.maxpool = nn.AdaptiveMaxPool1d(1)\n","    self.fc = nn.Linear(num_features, num_features*2)\n","    self.classifier = nn.Linear(num_features*2, 2)\n","    \n","    for m in self.modules():\n","      if isinstance(m, nn.BatchNorm1d):\n","        nn.init.constant_(m.weight, 0.5)\n","        nn.init.constant_(m.bias, 0)\n","      elif isinstance(m, nn.Linear):\n","        nn.init.constant_(m.bias, 0)\n","  \n","  def _make_block(self, outplanes, layer_count, leaky, self_attention):\n","    layers = []\n","    \n","    for i in range(0, layer_count):\n","      layers.append(resLayer(outplanes, leaky=leaky, self_attention = self_attention))\n","      if i < layer_count:\n","        layers.append(nn.Dropout(p=self.dropout, inplace = True))\n","    \n","    return nn.Sequential(*layers)\n","    \n","  def forward(self, x):\n","    x = self.embedding(x).unsqueeze(1)  \n","    \n","    for i, layer in enumerate(self.features):\n","      if i == 1: x = x.squeeze(3)\n","      x = layer(x)\n","    x = self.maxpool(x).view(x.size(0), -1)\n","    x = self.fc(x)\n","    x = self.leakyrelu(x)\n","    x = self.classifier(x) \n","    \n","    return x"],"id":"1855f521","execution_count":null,"outputs":[]}]}