{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f456e827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import fastai.layers as L\n",
    "\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e925c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_initializer(weight_matrix, non_trainable = True):\n",
    "    \n",
    "    num_embeds, dim_embeds = weight_matrix.shape\n",
    "    weight_matrix = torch.from_numpy(weight_matrix)\n",
    "    \n",
    "    embed_layer = nn.Embedding(num_embeds, dim_embeds, padding_idx = 1).to('cuda')\n",
    "    embed_layer.load_state_dict({'weight' : weights_matrix})\n",
    "    \n",
    "    embed_layer.weight.requires_grad = False if non_trainable else True\n",
    "    \n",
    "    return embed_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7770060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Residual_Block(input_channels, num_layers, self_attention = True, bottle = False, \n",
    "                   leaky = True):\n",
    "    \n",
    "    channel = input_channels\n",
    "    \n",
    "    layers = []\n",
    "    \n",
    "    conv_kwargs = {'is_1d': True, 'self_attention': self_attention, 'leaky' : leaky}\n",
    "    \n",
    "    for i in range(0, num_layers):\n",
    "        layers.append(L.res_block(channel, dense = True, bottle = bottle, **conv_kwargs))\n",
    "        layers.append(nn.Dropout(inplace = True))\n",
    "    \n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67a984fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottle_Drop(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_channels):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.BottleNeck = L.conv1d(input_channels, input_channels * 2, stride = 2)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \n",
    "        X_dropped = F.dropout(X, training = self.training)\n",
    "        \n",
    "        return X_dropped + self.BottleNeck(X.orig)\n",
    "    \n",
    "def Trans_Block(input_channels):\n",
    "    \n",
    "    t_block = L.SequentialEx\n",
    "    (\n",
    "    L.conv1d(input_channels, input_channels * 2, ks = 3, stride = 2, padding = 1),\n",
    "    Bottle_Drop(input_channels)\n",
    "    )\n",
    "    \n",
    "    return t_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f47f581",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResDongNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, weights_matrix, layers = (3, 4, 5, 3), \n",
    "                 input_channel = 16, bottle=False, embed_dim = 50, \n",
    "                 static = True):\n",
    "        \n",
    "        super(Residual, self).__init__(**kwargs)\n",
    "        \n",
    "        self.embedding = embed_initializer(weights_matrix, static)\n",
    "        self.init_conv = L.conv1d(1, input_channel, ks = (3, embed_dim), \n",
    "                                  stride = 1, padding = (1,0), bias = False)\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "        ('Conv1d', self.init_conv),\n",
    "        ('BatchNorm', nn.BatchNorm1d(input_channel)),\n",
    "        ('LeakyReLU', nn.LeakyReLU(inplace=True))\n",
    "        )\n",
    "        \n",
    "        for no, layer in enumerate(layers):\n",
    "            self.features.add_module('Residual_Block %d' %(i+1),\n",
    "                                    self.Residual_Block(input_channels, num_layers))\n",
    "            \n",
    "            if i < len(layers):\n",
    "                self.features.add_module('Trans_Block %d' %(i+1),\n",
    "                                        Trans_Block(input_channel))\n",
    "                \n",
    "        self.LeakyReLU = L.LeakyReLU(inplace=True)\n",
    "        self.Avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.FC1 = nn.Linear(input_channel, input_channel * 2)\n",
    "        self.classifier = nn.Linear(input_channel * 2, input_channel * 4)\n",
    "        \n",
    "        for mod in self.modules():\n",
    "            if isinstance(mod, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            \n",
    "            elif isinstance(mod, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                \n",
    "    def forward(self, X):\n",
    "        \n",
    "        X = self.embedding(X).unsqueeze(1)\n",
    "        \n",
    "        for no, layer in enumerate(self.features):\n",
    "            if i == 1:\n",
    "                X = X.squeeze(3)\n",
    "            X = layer(X)\n",
    "            \n",
    "        X = self.Avgpool(X).view(x.size(0), -1)\n",
    "        X = self.FC1(X)\n",
    "        X = self.LeakyReLU(x)\n",
    "        X = self.classifier(X)\n",
    "        \n",
    "        return X"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
