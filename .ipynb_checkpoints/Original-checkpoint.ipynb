{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1855f521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import fastai.layers as L\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def create_embedding_layer(weights_matrix, non_trainable=True):\n",
    "\n",
    "    \"\"\"Creates the embedding layer and loads the pre-trained embeddings from weights_matrix\n",
    "       non-trainable = True : static embeddings\n",
    "       non-trainable = False : non-static embeddings\n",
    "       \"\"\"\n",
    "\n",
    "    num_embeddings, embedding_dim = weights_matrix.shape\n",
    "\n",
    "    weights_matrix = torch.from_numpy(weights_matrix)   # Creates tensor from numpy array \n",
    "\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim, padding_idx=1).to('cuda')\n",
    "    # Padding index is kept zero and no gradients tracked\n",
    "    emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "    # Loads pretrained vector weights\n",
    "\n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "\n",
    "    else:\n",
    "        emb_layer.weight.require_grad = True\n",
    "\n",
    "    return emb_layer\n",
    "\n",
    "class dropsampleMergeLayer(nn.Module):\n",
    "    \"\"\"Mergelayer with dropout and downsampling\n",
    "        used within the transition blocks\"\"\"\n",
    "    def __init__(self, inplanes, dense: bool = False, dropout = 0):\n",
    "        super().__init__()\n",
    "        self.downsample = L.conv1d(inplanes, inplanes*2, stride=2)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.dropout > 0:\n",
    "            x_drop = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        return (x_drop + self.downsample(x.orig)) if self.dropout > 0 else (x + self.downsample(x.orig))\n",
    "\n",
    "\n",
    "def transition(inplanes, dropout = 0):\n",
    "\n",
    "    return L.SequentialEx(\n",
    "        L.conv1d(inplanes, inplanes*2, ks = 3, stride=2, padding = 1),\n",
    "        dropsampleMergeLayer(inplanes, dropout = dropout)\n",
    "    )\n",
    "\n",
    "def resLayer(inplanes, self_attention = False, bottle = False, leaky=None):\n",
    "\n",
    "    conv_kwargs = {'is_1d' : True, 'self_attention' : self_attention, 'leaky' : leaky}\n",
    "    \n",
    "    return L.res_block(inplanes, dense=False, bottle=bottle, **conv_kwargs)\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, weights_matrix, layers = (2, 2, 5, 5), inplanes = 16, bottle=False, zero_init_residual=False, dropout = 0, leaky = None, embed_dim = 50, static = False):\n",
    "\n",
    "        super(ResNet, self).__init__()\n",
    "\n",
    "        self.embedding = create_embedding_layer(weights_matrix, static)\n",
    "        self.init_conv = L.conv1d(1, inplanes, ks=(3, embed_dim), stride=1, \n",
    "                                  padding=(1,0), bias=False)\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        self.features = nn.Sequential(OrderedDict([\n",
    "        ('init_conv', self.init_conv),\n",
    "        ('init_norm', nn.BatchNorm1d(inplanes)),\n",
    "        ('init_relu', nn.ReLU(inplace=True))]))\n",
    "        \n",
    "        num_features = inplanes\n",
    "        for i, layer in enumerate(layers):\n",
    "            self.features.add_module('resblock%d' %(i+1), \n",
    "                                     self._make_block(num_features, layer, leaky=leaky))\n",
    "            if i!= len(layers)-1:\n",
    "                self.features.add_module('transition%d' %(i+1), \n",
    "                                         transition(num_features, dropout = self.dropout))                         \n",
    "                num_features = num_features*2\n",
    "        self.relu = L.relu(inplace=True, leaky=leaky)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc1 = nn.Linear(num_features, num_features*2)\n",
    "        self.classifier = nn.Linear(num_features*2, 2)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_block(self, outplanes, layer_count, leaky=None):\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        for i in range(0, layer_count):\n",
    "            layers.append(resLayer(outplanes, leaky=leaky))\n",
    "            if self.dropout > 0 and i != layer_count-1:\n",
    "                layers.append(nn.Dropout(p=self.dropout, inplace = True))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.embedding(x).unsqueeze(1)  \n",
    "                  \n",
    "        for i, layer in enumerate(self.features):\n",
    "            if i == 1:\n",
    "                x = x.squeeze(3)\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.avgpool(x).view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.classifier(x) \n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dbc863",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
